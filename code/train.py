# -*- coding: utf-8 -*-
"""image_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vritblXCND9H3H9pNi7URxqQIbJkuBMi
"""

from imblearn.over_sampling import SMOTE
smt = SMOTE()

# Commented out IPython magic to ensure Python compatibility.
import os, random
import numpy as np
import pandas as pd
import PIL
import keras
import itertools
from PIL import Image
from botocore.client import Config


from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from skimage import feature, data, io, measure
from sklearn.metrics import confusion_matrix

import matplotlib.pyplot as plt
from matplotlib import ticker
import seaborn as sns
# %matplotlib inline 

from keras import backend as K
from keras.models import Sequential
from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation,BatchNormalization
from keras.optimizers import RMSprop
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping
from keras.optimizers import Adam
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation
from keras.optimizers import RMSprop
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
import sys
import types
import pandas as pd

d1 = pd.read_csv("../data/gicsd_labels.csv") 
# Preview the first 5 lines of the loaded data
d2=d1[' LABEL']
k=0
for i in d2:
  #print(i)
  if(i.strip()=='FULL_VISIBILITY'):
    d2[k]=0
  elif(i.strip()=='PARTIAL_VISIBILITY'):
    d2[k]=1
  elif(i.strip()=='NO_VISIBILITY'):
    d2[k]=2
  k=k+1
print(d2.head())
print(d2.tail())
print(d2.unique())

from glob2 import glob
import pandas as pd


df = pd.DataFrame(columns=["file", "label"])
k=0
for image in glob("../data/images/*.png"):
    dir_ = image.split('/')
    file_= dir_[-1]

    df = df.append({
        "file": file_,
        "label": d2[k]
        }, ignore_index=True)
    k=k+1
print(df.head())
df.to_csv('data1.csv', index=False)

data = pd.read_csv('data1.csv')
print(data.head())

import matplotlib.pyplot as plt
import cv2
from sklearn.utils import shuffle
from keras.utils import np_utils

def shuffle_data(data):
    data = shuffle(data)#,random_state=2)
    return data
k=1

root_dir='../data/images'

def generator(samples, batch_size=40):
   
    global k
    num_samples = len(samples)
    while True: # Loop forever so the generator never terminates
        samples = shuffle(samples)

        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]
        for offset in range(0, num_samples, batch_size):
            # Get the samples you'll use in this batch
            batch_samples = samples[offset:offset+batch_size]
            # print(batch_samples)
            # Initialise X_train and y_train arrays for this batch
            X_train = []
            y_train = []
            # print('debug')
            # For each example
            for i in batch_samples.index:
                # Load image (X) and label (y)
                # print(batch_sample)
                img_name = batch_samples['file'][i]
                # print(img_name)
                label = batch_samples['label'][i]
                img =  cv2.imread(os.path.join(root_dir,img_name))
                # if(k==1):
                #   print('Resized Dimensions : ', img.shape)
                #   cv2.imshow("Resized image", img)  
                #   k=k+1
                # apply any kind of preprocessing
                #print(img.shape())
                # cv2.resize(img, dsize=(192, 192), interpolation=cv2.INTER_LINEAR)
                img = img[:,:,0]
                img.reshape(192,192,1)
                # Add example to arrays
                X_train.append(img)
                y_train.append(label)
                # print('debug')

            # Make sure they're numpy arrays (as opposed to lists)
            # X_train, y_train = smt.fit_sample(X_train, y_train)
            X_train = np.array(X_train)
            y_train = np.array(y_train)
           

            # The generator-y part: yield the next training batch 
            print(X_train,y_train)           
            yield X_train, y_train

k=1
samples=pd.read_csv('data1.csv')
samples.head()
train_datagen = generator(samples,batch_size=40)

x,y = next(train_datagen)

print ('x_shape: ', x.shape)
print ('labels: ', y)

fig = plt.figure(1,figsize=(12,12))
for i in range(8):
  plt.subplot(4,4,i+1)
  plt.tight_layout()
  #x[i] = x[i][:,::-1] # converting BGR to RGB
  plt.imshow(x[i][:,::-1], interpolation='none')
  plt.title("class_label: {}".format(y[i]))
  plt.xticks([])
  plt.yticks([])
plt

def preprocessing(img,label):
    resize=192
    img = img[:,:,0]
    img = img.reshape(192,192,1)
    img = img/255
    label = np_utils.to_categorical(label, num_classes=3)
    return img,label

def data_generator(samples, batch_size=40,shuffle_data=True,resize=192):
    
    # from google.colab.patches import cv2_imshow
    num_samples = len(samples)
    while True: # Loop forever so the generator never terminates
        samples = shuffle(samples)

        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]
        for offset in range(0, num_samples, batch_size):
            # Get the samples you'll use in this batch
            batch_samples = samples[offset:offset+batch_size]

            # Initialise X_train and y_train arrays for this batch
            X_train = []
            y_train = []

            # For each example
            for i in batch_samples.index:
                # Load image (X) and label (y)
                img_name = batch_samples['file'][i]
                # print(img_name)
                label = batch_samples['label'][i]
                img =  cv2.imread(os.path.join(root_dir,img_name))
                # print('Resized Dimensions : ', img.shape)  
                # apply any kind of preprocessing
                img,label = preprocessing(img,label)
                # print(img.shape)
                # Add example to arrays
                X_train.append(img)
                y_train.append(label)

            # Make sure they're numpy arrays (as opposed to lists)
            # X_train, y_train = smt.fit_sample(X_train, y_train)
            X_train = np.array(X_train)
            y_train = np.array(y_train)
            

            # The generator-y part: yield the next training batch            
            yield X_train, y_train

train_datagen = data_generator(samples,batch_size=8)

x,y = next(train_datagen)
print ('x_shape: ', x.shape)
print ('labels shape: ', y.shape)
print ('labels: ', y)

dd=pd.read_csv('data1.csv')
dd['split'] = np.random.randn(dd.shape[0], 1)
msk = np.random.rand(len(dd)) <= 0.8

train = dd[msk]
test = dd[~msk]
num_train = len(train)
num_test = len(test)

print ('number of train samples: ',num_train )
print ('number of test samples: ', num_test)

from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, Activation, MaxPooling2D, Dropout

input_shape=(192,192,1)
print (input_shape)

model = Sequential()
#filters,kernel_size,strides=(1, 1),padding='valid',data_format=None,dilation_rate=(1, 1),activation=None,use_bias=True,
#kernel_initializer='glorot_uniform',bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,
#activity_regularizer=None,kernel_constraint=None,bias_constraint=None,

#pool_size=(2, 2), strides=None, padding='valid',data_format=None

model.add(Conv2D(32, (3,3),padding='same',activation='relu',input_shape=input_shape,name='conv2d_1'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_1'))
model.add(Conv2D(32, (3,3),padding='same',activation='relu',name='conv2d_2'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Conv2D(64, (3, 3),activation='relu',name='conv2d_3'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_2'))

model.add(Dropout(0.5))

model.add(Conv2D(128, (3, 3),padding='same',activation='relu',name='conv2d_5'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3, 3),padding='same',activation='relu',name='conv2d_6'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Conv2D(128, (3, 3),padding='same',activation='relu',name='conv2d_7'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))


model.add(Flatten())
model.add(Dense(32))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(3,activation='sigmoid'))

model.summary()

from keras.optimizers import Adam
opt = Adam(lr=0.0005)
model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

batch_size = 40
train_generator = data_generator(train, batch_size=8)
validation_generator = data_generator(test, batch_size=8)

model.fit_generator(
        train_generator,
        steps_per_epoch=num_train // batch_size,
        epochs=50,
        validation_data=validation_generator,
        validation_steps=num_test // batch_size)
model.save_weights('../artifacts/first_try.h5')

